{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import rosbag\n",
    "import matplotlib.pyplot as plt\n",
    "import data_conversion\n",
    "import depth_anything_interface\n",
    "import pcd_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-10-16 02:30:40,251 - dinov2 - using MLP layer as FFN\n"
     ]
    }
   ],
   "source": [
    "FRAME_INDEX = 0\n",
    "GAP_INDEX = 10\n",
    "MODEL_PATH = \"/scratchdata/depth_anything_v2_metric_hypersim_vitl.pth\"\n",
    "model = depth_anything_interface.get_model(\"cuda\", MODEL_PATH, model_type = \"metric\", encoder='vitl')\n",
    "\n",
    "config = yaml.load(open(\"../config/gemini2L.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "# Open bag file\n",
    "bag_file_path = \"/scratchdata/indoor_short.bag\"\n",
    "bag = rosbag.Bag(bag_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306.4570007324219 306.4668884277344 319.01312255859375 197.51637268066406\n"
     ]
    }
   ],
   "source": [
    "wait = 0\n",
    "for topic, msg, t in bag.read_messages(topics=[\"/camera/color/camera_info\"]):\n",
    "    D = msg.D\n",
    "    K = msg.K\n",
    "    R = msg.R\n",
    "    P = msg.P\n",
    "    break\n",
    "\n",
    "fx = P[0]\n",
    "fy = P[5]\n",
    "cx = P[2]\n",
    "cy = P[6]\n",
    "\n",
    "print(fx, fy, cx, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0 \n",
    "for topic, msg, t in bag.read_messages(topics=[\"/camera/color/image_raw\"]):\n",
    "    if cnt ==  FRAME_INDEX:\n",
    "        prev_img = data_conversion.topic_to_image(msg)\n",
    "    if cnt == GAP_INDEX + FRAME_INDEX:\n",
    "        new_img = data_conversion.topic_to_image(msg)\n",
    "        break\n",
    "    cnt+=1\n",
    "\n",
    "cnt = 0 \n",
    "for topic, msg, t in bag.read_messages(topics=[\"/camera/depth/image_raw\"]):\n",
    "    if cnt ==  FRAME_INDEX:\n",
    "        prev_depth = data_conversion.topic_to_depth(msg, config[\"depth_anything_config\"])\n",
    "    if cnt == GAP_INDEX + FRAME_INDEX:\n",
    "        new_depth = data_conversion.topic_to_depth(msg, config[\"depth_anything_config\"])\n",
    "        break\n",
    "    cnt+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev_est_depth = prev_depth\n",
    "#new_est_depth = new_depth * 0.9\n",
    "\n",
    "prev_est_depth = model.infer_image(prev_img)\n",
    "new_est_depth = model.infer_image(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62933, 3) (62933, 3)\n"
     ]
    }
   ],
   "source": [
    "gray_prev = cv2.cvtColor(prev_img, cv2.COLOR_BGR2GRAY)\n",
    "gray_new = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "flow = cv2.calcOpticalFlowFarneback(gray_prev, gray_new, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "prev_point = np.indices((prev_img.shape[0], prev_img.shape[1]))\n",
    "prev_point = np.moveaxis(prev_point, 0, -1)\n",
    "\n",
    "new_point = prev_point + flow\n",
    "\n",
    "mask = np.linalg.norm(flow, axis=2) > 20 # Match based on nearer objects, these should be more accurate?\n",
    "prev_point = prev_point[mask==1]\n",
    "new_point = new_point[mask==1]\n",
    "\n",
    "mask = new_point[:, 1] < new_img.shape[0] - 1 \n",
    "prev_point = prev_point[mask]\n",
    "new_point = new_point[mask]\n",
    "\n",
    "mask = prev_point[:, 1] < new_img.shape[0] - 1\n",
    "prev_point = prev_point[mask]\n",
    "new_point = new_point[mask]\n",
    "\n",
    "mask = new_point[:, 0] < new_img.shape[1] - 1 \n",
    "prev_point = prev_point[mask]\n",
    "new_point = new_point[mask]\n",
    "\n",
    "mask = prev_point[:, 0] < new_img.shape[1] - 1\n",
    "prev_point = prev_point[mask]\n",
    "new_point = new_point[mask]\n",
    "\n",
    "matching_depth1 = data_conversion.interpolate_depth(prev_est_depth, prev_point)\n",
    "matching_depth2 = data_conversion.interpolate_depth(new_est_depth, new_point)\n",
    "\n",
    "coord1 = np.zeros((len(matching_depth1), 3), dtype=np.float32)\n",
    "\n",
    "coord1[:, 0] = (prev_point[:,0] - cx) * matching_depth1/ fx\n",
    "coord1[:, 1] = (prev_point[:,1] - cy) * matching_depth1/ fy\n",
    "coord1[:, 2] = matching_depth1\n",
    "\n",
    "coord2 = np.zeros((len(matching_depth2), 3), dtype=np.float32)\n",
    "\n",
    "coord2[:, 0] = (new_point[:,0] - cx) * matching_depth2/ fx\n",
    "coord2[:, 1] = (new_point[:,1] - cy) * matching_depth2/ fy\n",
    "coord2[:, 2] = matching_depth2\n",
    "\n",
    "print(coord1.shape, coord2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2363/3434762136.py:30: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  scale = np.einsum('ij,ij->', sample1, sample2) / np.einsum('ij,ij->', sample2, sample2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5132601337930816\n",
      "[[ 0.99656808  0.00106347 -0.08276974 -0.01713545]\n",
      " [-0.00585026  0.99832195 -0.05761163  0.02939722]\n",
      " [ 0.08256958  0.05789815  0.99490201  0.05235686]\n",
      " [ 0.          0.          0.          0.90407683]]\n"
     ]
    }
   ],
   "source": [
    "EPISON = 0.1\n",
    "RANSAC_ITERATIONS = 50000\n",
    "\n",
    "best_inliers = 0\n",
    "best_tf = np.eye(4)\n",
    "\n",
    "for _ in range(RANSAC_ITERATIONS):\n",
    "    sample_indices = np.random.choice(len(coord1), 3, replace=False)\n",
    "    sample1 = coord1[sample_indices]\n",
    "    sample2 = coord2[sample_indices]\n",
    "\n",
    "    # Zero Mean\n",
    "    sample1_mean = np.mean(sample1, axis=0)\n",
    "    sample2_mean = np.mean(sample2, axis=0)\n",
    "\n",
    "    sample1 = sample1 - sample1_mean\n",
    "    sample2 = sample2 - sample2_mean\n",
    "\n",
    "    # SVD\n",
    "    H = sample2.T @ sample1\n",
    "    try:\n",
    "        U, D, V_T = np.linalg.svd(H)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    R = U @ V_T\n",
    "\n",
    "    sample2 = sample2 @ R.T\n",
    "\n",
    "    scale = np.einsum('ij,ij->', sample1, sample2) / np.einsum('ij,ij->', sample2, sample2)\n",
    "    #print(np.einsum('ij,ij->', sample1, sample2), np.einsum('ij,ij->', sample2, sample2))\n",
    "\n",
    "    if scale < 1e-5:\n",
    "        continue\n",
    "        \n",
    "    test_coord2 = (coord2 - sample2_mean) * scale\n",
    "    test_coord2 = test_coord2 @ R.T + sample1_mean\n",
    "\n",
    "    error = np.linalg.norm(test_coord2 - coord1, axis=1)\n",
    "\n",
    "    inliers = np.mean(error < EPISON)\n",
    "\n",
    "    if (inliers > best_inliers):\n",
    "        best_inliers = inliers\n",
    "\n",
    "        tf = np.eye(4)\n",
    "        tf[:3,3] = -sample2_mean.T\n",
    "\n",
    "        new_tf = np.eye(4)\n",
    "        new_tf[3,3] = 1/scale\n",
    "        tf = new_tf @ tf\n",
    "\n",
    "        new_tf = np.eye(4)\n",
    "        new_tf[:3,:3] = R\n",
    "        tf = new_tf @ tf\n",
    "\n",
    "        new_tf = np.eye(4)\n",
    "        new_tf[:3,3] = sample1_mean.T\n",
    "        tf = new_tf @ tf\n",
    "\n",
    "        best_tf = tf\n",
    "\n",
    "print(best_inliers)\n",
    "print(best_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 10172 points."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_coord = data_conversion.depth_to_pcd(prev_est_depth,P) \n",
    "#prev_coord = prev_coord - mean_ori\n",
    "#prev_coord = prev_coord / scale_ori\n",
    "\n",
    "prev_coord = np.concatenate([prev_coord, np.ones((prev_coord.shape[0], 1))], axis=1)\n",
    "\n",
    "prev_pcd = o3d.geometry.PointCloud()\n",
    "prev_pcd.points = o3d.utility.Vector3dVector(prev_coord[:,:3]/prev_coord[:,3][:,None])\n",
    "prev_pcd.colors = o3d.utility.Vector3dVector(prev_img.reshape(-1,3)/255.0)\n",
    "\n",
    "#prev_pcd = prev_pcd.uniform_down_sample(every_k_points=4)\n",
    "prev_pcd.voxel_down_sample(0.1)\n",
    "\n",
    "\n",
    "new_coord = data_conversion.depth_to_pcd(new_est_depth,P)\n",
    "#new_coord = new_coord - mean_new\n",
    "#new_coord = new_coord / scale_new\n",
    "\n",
    "new_coord = np.concatenate([new_coord, np.ones((new_coord.shape[0], 1))], axis=1)\n",
    "new_coord = new_coord @ best_tf.T\n",
    "\n",
    "new_pcd = o3d.geometry.PointCloud()\n",
    "new_pcd.points = o3d.utility.Vector3dVector(new_coord[:,:3]/new_coord[:,3][:,None])\n",
    "new_pcd.colors = o3d.utility.Vector3dVector(new_img.reshape(-1,3)/255.0)\n",
    "\n",
    "#new_pcd = new_pcd.uniform_down_sample(every_k_points=4)\n",
    "new_pcd.voxel_down_sample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([prev_pcd, new_pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Colored point cloud registration\n",
      "[50, 0.04, 0]\n",
      "3-1. Downsample with a voxel size 0.04\n",
      "3-2. Estimate normal.\n",
      "3-3. Applying colored point cloud registration\n",
      "RegistrationResult with fitness=6.219243e-01, inlier_rmse=2.273578e-02, and correspondence_set size of 21383\n",
      "Access transformation to get result.\n",
      "[30, 0.02, 1]\n",
      "3-1. Downsample with a voxel size 0.02\n",
      "3-2. Estimate normal.\n",
      "3-3. Applying colored point cloud registration\n",
      "RegistrationResult with fitness=4.784641e-01, inlier_rmse=1.224865e-02, and correspondence_set size of 36447\n",
      "Access transformation to get result.\n",
      "[14, 0.01, 2]\n",
      "3-1. Downsample with a voxel size 0.01\n",
      "3-2. Estimate normal.\n",
      "3-3. Applying colored point cloud registration\n",
      "RegistrationResult with fitness=2.410612e-01, inlier_rmse=6.763303e-03, and correspondence_set size of 31485\n",
      "Access transformation to get result.\n"
     ]
    }
   ],
   "source": [
    "voxel_radius = [0.04, 0.02, 0.01]\n",
    "max_iter = [50, 30, 14]\n",
    "current_transformation = np.identity(4)\n",
    "print(\"3. Colored point cloud registration\")\n",
    "for scale in range(3):\n",
    "    iter = max_iter[scale]\n",
    "    radius = voxel_radius[scale]\n",
    "    print([iter, radius, scale])\n",
    "\n",
    "    print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "    source_down = prev_pcd.voxel_down_sample(radius)\n",
    "    target_down = new_pcd.voxel_down_sample(radius)\n",
    "\n",
    "    print(\"3-2. Estimate normal.\")\n",
    "    source_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "    target_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "    print(\"3-3. Applying colored point cloud registration\")\n",
    "    result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "        source_down, target_down, radius, current_transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationForColoredICP(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                          relative_rmse=1e-6,\n",
    "                                                          max_iteration=iter))\n",
    "    current_transformation = result_icp.transformation\n",
    "    print(result_icp)\n",
    "\n",
    "o3d.visualization.draw_geometries([prev_pcd, new_pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
